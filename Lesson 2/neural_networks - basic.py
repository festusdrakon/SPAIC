# -*- coding: utf-8 -*-
"""neural networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egCodulhWRx7bCuLgVC31dVG4zbRcVYf
"""

# Import necessary packages

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import numpy as np
import torch

import helper

import matplotlib.pyplot as plt

from torchvision import datasets, transforms

# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),
                              transforms.Normalize((0.5,), (0.5,)),
                              ])

# Download and load the training data
trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# to loop through the batch
dataiter = iter(trainloader)
images, labels = dataiter.next()
print(type(images))
print(images.shape)
print(labels.shape)

plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');

#imageIter = iter(images).flatten()
#flattened_images = images.flatten()

flattened_images= images.view(images.shape[0], -1)  #64x784

print(flattened_images.shape)

flattened_images.shape

def activation(x):
  # sigmoid activation function
    return (1/ (1 + torch.exp(-x)))
  
def softmax(x):
    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)

## Your solution

number_input= 784
number_output= 10
number_hidden = 256

weights_0_1 = torch.randn(number_input, number_hidden)  # 784x256
weights_1_2 = torch.randn(number_hidden, number_output)    # 256x10

bias_1= torch.randn((1,number_hidden))  #1x256
bias_2 = torch.randn((1,number_output)) #1x10

output_layer_1= activation(torch.mm(flattened_images,weights_0_1)+ bias_1)
output_layer_2= (torch.mm(output_layer_1, weights_1_2)+ bias_2)

#Softmax activation function - defined in torch nn module
softmax_function = torch.nn.Softmax(dim=1)

#Softmax activation function- defined above
output_softmax=softmax(output_layer_2)

print(output_softmax.sum(dim=1))
# output of your network, should have shape (64,10)

