# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MzDvnZzE0yO_XrXqTtrYqZKUArTL-Cbz
"""

import torch

def activation(x):
  # sigmoid activation function
  return (1/ (1 + torch.exp(-x)))

#generate fake data

seed= torch.manual_seed(7)

features = torch.randn((1,5))  #2-D tensor with 1 row and 5 columns
weights = torch.randn_like(features) #randn_like - to create a tensor with same dimensions
bias = torch.randn([1,1])

#print(features)
#print(weights)
#print(bias)

#mul_output = (torch.sum(features* weights)+ bias)

# change shape for correct multiplication
mul_torchmm_output = torch.mm(features, weights.view(5,1))+ bias

# incorrect multiplication - mul_notorch_output = ((features* weights)+ bias)

#print(mul_output)
#print(mul_torchmm_output)

final_output = activation(mul_torchmm_output)
print (final_output)

#Multi layer

# 3 inputs -> 1 output. Here we have a hidden layer, Layer 2. Layer 2 has 2 inputs which are the outputs from Layer 1.
# Layer 2 has it's own weights and bias.
# So first we calculate the first layer output, then 2nd layer output as final
torch.manual_seed(7)

features= torch.randn((1,3)) # 3 inputs
print(features)
number_input = features.shape[1] # returns number of columns. here it is 3.
number_output = 1
number_hidden =2

weights_layer1 = torch.randn(number_input, number_hidden)
weights_layer2 = torch.randn(number_hidden, number_output)

bias_1 = torch.randn((1,number_hidden))
bias_2 = torch.randn((1,number_output))

output_layer1 = (torch.mm(features, weights_layer1) + bias_1)
output_1 = activation(output_layer1)
output_layer2 = torch.mm(output_1, weights_layer2) + bias_2
output_final = activation(output_layer2)
print(output_final)

