# -*- coding: utf-8 -*-
"""nn_hidden layer_forward pass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W6ZWtmYXobwUeT2OO2ql0DO4SaJCD-3C
"""

# Import necessary packages

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import numpy as np
import torch
import matplotlib.pyplot as plt
from torchvision import datasets, transforms

# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),
                              transforms.Normalize((0.5,), (0.5,)),
                              ])

# Download and load the training data
trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# rewriting model using nn.Sequential
# this is a feed forward network
from torch import nn
model= nn.Sequential(nn.Linear(784, 128),
                     nn.ReLU(),
                     nn.Linear(128, 64),
                     nn.ReLU(),
                     nn.Linear(64, 10),
                     nn.LogSoftmax(dim=1)) #softmax function.Outputs will be probabilities, not logits

# defining our loss functin
criterion = nn.NLLLoss()   #No need to define separately.

#images and labels
images, labels = next(iter(trainloader))
flattened_images= images.view(images.shape[0], -1)  #64x784

# Pass images through model
flattened_images.requires_grad = True
log_probabilities = model(flattened_images)

#calculate loss of the model
#logits are our model's results
#labels are the correct answers
loss = criterion(log_probabilities, labels)

print(loss)

print(log_probabilities.grad)
print(log_probabilities.grad_fn)

print(loss.grad)
print(loss.grad_fn)

print('Before backward pass: \n', model[0].weight.grad)

loss.backward()

print('After backward pass: \n', model[0].weight.grad)



