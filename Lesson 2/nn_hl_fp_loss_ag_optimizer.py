# -*- coding: utf-8 -*-
"""nn_hl_fp_loss_ag_optimizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W6ZWtmYXobwUeT2OO2ql0DO4SaJCD-3C
"""

# Import necessary packages

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import numpy as np
import torch
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch import optim

# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),
                              transforms.Normalize((0.5,), (0.5,)),                              ])

# Download and load the training data
trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# rewriting model using nn.Sequential
# this is a feed forward network
from torch import nn
model= nn.Sequential(nn.Linear(784, 128),
                     nn.ReLU(),
                     nn.Linear(128, 64),
                     nn.ReLU(),
                     nn.Linear(64, 10),
                     nn.LogSoftmax(dim=1)) #softmax function.Outputs will be probabilities, not logits

# defining our loss functin
criterion = nn.NLLLoss()   #No need to define separately.

#initial weight
print(model[0].weight)

# Optimizers require the parameters to optimize and a learning rate
optimizer = optim.SGD(model.parameters(), lr=0.01)

#images and labels
images, labels = next(iter(trainloader))
flattened_images= images.view(images.shape[0], -1)  #64x784

# Pass images through model
flattened_images.requires_grad = True

log_probabilities = model(flattened_images)

#calculate loss of the model
#logits are our model's results
#labels are the correct answers
loss = criterion(log_probabilities, labels)

print(loss)

print(log_probabilities.grad)
print(log_probabilities.grad_fn)

print(loss.grad)
print(loss.grad_fn)

print('Before backward pass: \n', model[0].weight.grad)

loss.backward()

print('After backward pass: \n', model[0].weight.grad)

# Take an update step and few the new weights
print('Old weights - ', model[0].weight)
optimizer.step()
print('Updated weights - ', model[0].weight)

